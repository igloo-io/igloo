use anyhow::{anyhow, Context, Result};
// bincode is used via bincode::serialize/deserialize directly
use futures::future::try_join_all;
use std::future::Future; // For boxing futures
use std::pin::Pin;     // For boxing futures
use uuid::Uuid;        // For generating task IDs

// Corrected imports based on protobuf structure and engine module structure
use igloo_api::igloo::worker_service_client::WorkerServiceClient;
use igloo_api::igloo::TaskDefinition;
use igloo_engine::logical_plan::LogicalPlan as EngineLogicalPlan;
use igloo_engine::physical_plan::PhysicalPlan as EnginePhysicalPlan; // Re-corrected path
use sqlparser::ast::Statement;
use sqlparser::dialect::GenericDialect;
use sqlparser::parser::Parser;

/// Plans a SQL query and returns a serialized physical plan.
pub fn plan_query(sql: &str) -> Result<Vec<u8>> {
    let dialect = GenericDialect {};
    let ast_statements =
        Parser::parse_sql(&dialect, sql).map_err(|e| anyhow!("SQL parsing error: {:?}", e))?;

    let ast = ast_statements
        .into_iter()
        .next()
        .ok_or_else(|| anyhow!("No SQL statement provided to plan"))?;

    let _logical_plan = EngineLogicalPlan::Dummy;

    let physical_plan = match ast {
        Statement::Query(_) => EnginePhysicalPlan::Scan {
            table_name: "placeholder_table".to_string(),
            columns: vec!["*".to_string()],
            predicate: None,
            partition_id: None,
            total_partitions: None,
        },
        _ => EnginePhysicalPlan::Dummy,
    };

    bincode::serialize(&physical_plan).context("Failed to serialize physical plan")
}

/// Returns a hardcoded list of worker addresses.
pub fn get_worker_addresses() -> Vec<String> {
    vec![
        "http://localhost:50051".to_string(),
        "http://localhost:50052".to_string(),
        "http://localhost:50053".to_string(),
    ]
}

/// Splits a physical plan into sub-plans for parallel execution.
pub fn split_plan(plan: EnginePhysicalPlan, num_workers: usize) -> Vec<EnginePhysicalPlan> {
    if num_workers == 0 || num_workers == 1 {
        return vec![plan];
    }

    match plan {
        EnginePhysicalPlan::Scan {
            table_name,
            columns,
            predicate,
            ..
        } => {
            let mut split_plans = Vec::with_capacity(num_workers);
            for i in 0..num_workers {
                split_plans.push(EnginePhysicalPlan::Scan {
                    table_name: table_name.clone(),
                    columns: columns.clone(),
                    predicate: predicate.clone(),
                    partition_id: Some(i as u32),
                    total_partitions: Some(num_workers as u32),
                });
            }
            split_plans
        }
        _ => vec![plan.clone()],
    }
}

/// Dispatches a query to multiple workers in parallel after splitting the plan.
pub async fn dispatch_parallel_task(sql: &str) -> Result<()> {
    let worker_addresses = get_worker_addresses();
    let num_workers = worker_addresses.len();

    if num_workers == 0 {
        return Err(anyhow!("No workers available to dispatch tasks."));
    }

    let original_serialized_plan =
        plan_query(sql).context(format!("Failed to plan query for parallel dispatch: {}", sql))?;

    let original_physical_plan: EnginePhysicalPlan =
        bincode::deserialize(&original_serialized_plan)
            .context("Failed to deserialize original physical plan")?;

    let partitioned_plans = split_plan(original_physical_plan, num_workers);

    if partitioned_plans.is_empty() && num_workers > 0 {
         return Err(anyhow!("No plans generated by split_plan, but workers are available."));
    }
    // If it's a Scan plan, it should be split into num_workers parts.
    // If it's not a Scan plan, partitioned_plans will have 1 element.
    if let Some(EnginePhysicalPlan::Scan {..}) = partitioned_plans.first() {
        if partitioned_plans.len() != num_workers {
            return Err(anyhow!(
                "Scan plan splitting did not result in the expected number of sub-plans. Expected {}, got {}.",
                num_workers,
                partitioned_plans.len()
            ));
        }
    }


    let mut futures_vec: Vec<Pin<Box<dyn Future<Output = Result<(), anyhow::Error>> + Send>>> = Vec::new();

    if partitioned_plans.len() == 1 && num_workers > 0 {
        // Case: Non-scan plan, or scan plan for a single worker.
        // Dispatch this single plan to the first available worker.
        let plan_to_dispatch = partitioned_plans[0].clone();
        let worker_address = worker_addresses[0].clone();

        let future = Box::pin(async move {
            let task_id = Uuid::new_v4().to_string();
            let serialized_plan = bincode::serialize(&plan_to_dispatch)
                .context("Failed to serialize plan for dispatch")?;
            let task_definition = TaskDefinition {
                task_id,
                payload: serialized_plan // Corrected field name
            };

            let mut client = WorkerServiceClient::connect(worker_address.to_string()).await // Corrected client type
                .context(format!("Failed to connect to worker: {}", worker_address))?;
            client.execute_task(tonic::Request::new(task_definition)).await
                .context(format!("Failed to dispatch task to worker: {}", worker_address))?;
            Ok(())
        });
        futures_vec.push(future);
    } else {
        // Case: Scan plan split for multiple workers.
        for (i, partitioned_plan) in partitioned_plans.into_iter().enumerate() {
            // This ensures we don't panic if split_plan somehow returns more plans than workers for a Scan
            // (though current split_plan logic for Scan ensures partitioned_plans.len() == num_workers)
            if i >= num_workers {
                eprintln!("Warning: More partitioned plans generated than available workers. Skipping extra plans.");
                break;
            }
            let worker_address = worker_addresses[i].clone();

            let future = Box::pin(async move {
                let task_id = Uuid::new_v4().to_string();
                let serialized_plan = bincode::serialize(&partitioned_plan)
                    .context("Failed to serialize partitioned plan")?;
                let task_definition = TaskDefinition {
                    task_id,
                    payload: serialized_plan // Corrected field name
                };

                let mut client = WorkerServiceClient::connect(worker_address.to_string()).await // Corrected client type
                    .context(format!("Failed to connect to worker: {}", worker_address))?;
                client.execute_task(tonic::Request::new(task_definition)).await
                    .context(format!("Failed to dispatch task to worker: {}", worker_address))?;
                Ok(())
            });
            futures_vec.push(future);
        }
    }

    if futures_vec.is_empty() {
        if num_workers == 0 || sql.trim().is_empty() {
            println!("No tasks to dispatch (no workers or empty SQL).");
            return Ok(());
        } else {
            // This case implies plans were generated but no futures were created, which would be a logic error.
            return Err(anyhow!("No tasks were prepared for dispatch despite available workers and non-empty SQL."));
        }
    }

    match try_join_all(futures_vec).await {
        Ok(_) => {
            println!("All tasks dispatched successfully to workers.");
            Ok(())
        }
        Err(e) => {
            eprintln!("An error occurred while dispatching tasks: {:?}", e);
            Err(e)
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use igloo_engine::logical_plan::Expression as EngineExpression;

    #[test]
    fn test_plan_query_simple_select() {
        let sql = "SELECT a FROM my_table";
        let result = plan_query(sql);
        assert!(result.is_ok(), "plan_query failed for simple select: {:?}", result.err());
        let serialized_plan = result.unwrap();
        assert!(!serialized_plan.is_empty());

        let plan: Result<EnginePhysicalPlan, _> = bincode::deserialize(&serialized_plan);
        assert!(plan.is_ok(), "Failed to deserialize plan from plan_query");
        match plan.unwrap() {
            EnginePhysicalPlan::Scan { table_name, .. } => {
                assert_eq!(table_name, "placeholder_table")
            }
            _ => panic!("Expected a Scan plan for SELECT query"),
        }
    }

    #[test]
    fn test_split_scan_plan() {
        let scan_plan = EnginePhysicalPlan::Scan {
            table_name: "test_table".to_string(),
            columns: vec!["col1".to_string(), "col2".to_string()],
            predicate: Some(EngineExpression::Dummy),
            partition_id: None,
            total_partitions: None,
        };

        let num_workers = 3;
        let split_plans = split_plan(scan_plan, num_workers);

        assert_eq!(split_plans.len(), num_workers);
        for (i, p) in split_plans.iter().enumerate() {
            if let EnginePhysicalPlan::Scan { partition_id, total_partitions, .. } = p {
                assert_eq!(*partition_id, Some(i as u32));
                assert_eq!(*total_partitions, Some(num_workers as u32));
            } else {
                panic!("Expected a Scan plan after splitting");
            }
        }
    }

    #[test]
    fn test_split_non_scan_plan() {
        let dummy_plan = EnginePhysicalPlan::Dummy;
        let num_workers = 3;
        let split_plans = split_plan(dummy_plan.clone(), num_workers);

        assert_eq!(split_plans.len(), 1);
        assert_eq!(split_plans[0], dummy_plan);
    }

    #[test]
    fn test_split_plan_with_one_worker() {
        let scan_plan = EnginePhysicalPlan::Scan {
            table_name: "test_table".to_string(),
            columns: vec!["col1".to_string()],
            predicate: None,
            partition_id: None,
            total_partitions: None,
        };
        let num_workers = 1;
        let split_plans = split_plan(scan_plan.clone(), num_workers);
        assert_eq!(split_plans.len(), 1);
        if let EnginePhysicalPlan::Scan { partition_id, total_partitions, .. } = &split_plans[0] {
            assert!(partition_id.is_none());
            assert!(total_partitions.is_none());
        } else {
            panic!("Expected a Scan plan");
        }
    }

    #[test]
    fn test_split_plan_with_zero_workers() {
        let scan_plan = EnginePhysicalPlan::Scan {
            table_name: "test_table".to_string(),
            columns: vec!["col1".to_string()],
            predicate: None,
            partition_id: None,
            total_partitions: None,
        };
        let num_workers = 0;
        let split_plans = split_plan(scan_plan.clone(), num_workers);
        assert_eq!(split_plans.len(), 1);
        if let EnginePhysicalPlan::Scan { partition_id, total_partitions, .. } = &split_plans[0] {
            assert!(partition_id.is_none());
            assert!(total_partitions.is_none());
        } else {
            panic!("Expected a Scan plan");
        }
    }
}
