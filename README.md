# ğŸ™ Igloo: The Distributed SQL Query Engine

Igloo is a high-performance, distributed SQL query engine built in Rust that makes data access simple, fast, and intelligent. Query data from multiple sourcesâ€”operational databases, data lakes, and streaming systemsâ€”through a single, unified SQL interface.

---

## ğŸš€ Quickstart

1. **Install Prerequisites:**
   * [Rust (latest stable)](https://www.rust-lang.org/tools/install)
   * [Protocol Buffers Compiler (`protoc`)](https://grpc.io/docs/protoc-installation/)
   * (Optional) [Python 3.x](https://www.python.org/downloads/) for Python bindings

2. **Clone and Build:**
   ```bash
   git clone https://github.com/igloo-io/igloo
   cd igloo
   cargo build --release
   ```

3. **Quick Start with Docker:**
   ```bash
   docker-compose up -d --build
   ```

4. **Run Tests:**
   ```bash
   cargo test
   ```

---

## âœ¨ Core Features

* **ğŸ”— Federated Queries**: Connect multiple data sources (PostgreSQL, MySQL, data lakes) and query them together in a single SQL statement
* **âš¡ Powered by Apache Arrow DataFusion**: Lightning-fast, extensible Rust-native query engine with rich optimizations
* **ğŸš€ High-Speed Transport**: Apache Arrow Flight SQL for client-server communicationâ€”significantly faster than ODBC/JDBC
* **ğŸ§  Intelligent Caching**: Transparent caching layer with automatic cache invalidation via Change Data Capture (CDC)
* **ğŸ“ˆ Elastic & Scalable**: Distributed architecture that scales from a single laptop to thousands of nodes
* **ğŸ›¡ï¸ Memory Safe**: Written in Rust for guaranteed memory safety and high reliability
* **ğŸ”Œ Extensible**: Modular connector architecture makes adding new data sources trivial

---

## ğŸ—ï¸ Architecture Overview

Igloo uses a simple yet powerful coordinator-worker architecture:

### ğŸ§  The Coordinator Node
The brain of the cluster, responsible for:
- **Client Connections**: Arrow Flight SQL endpoint for SQL query submission
- **Query Planning**: Apache Arrow DataFusion-powered SQL parsing, planning, and optimization
- **Smart Routing**: Decides between live database queries and cached data
- **Cluster Management**: Real-time worker tracking and intelligent task scheduling

### ğŸ‘· Worker Nodes
The hands of the cluster, each worker:
- **Registers** with the Coordinator announcing available resources
- **Executes** assigned query tasks using specialized connectors
- **Processes** data in-memory with high-performance query engine
- **Communicates** results between workers and back to the Coordinator

### ğŸ”„ Query Execution Flow

```mermaid
sequenceDiagram
    participant Client
    participant Coordinator
    participant Worker1
    participant Worker2
    participant Cache/DB

    Client->>Coordinator: SQL Query via Flight SQL
    Coordinator->>Coordinator: Parse & Plan (DataFusion)
    Coordinator->>Coordinator: Check Cache Strategy
    
    par Distributed Execution
        Coordinator->>Worker1: Task 1: Scan data partition A
        Coordinator->>Worker2: Task 2: Scan data partition B
    end
    
    par Data Fetching
        Worker1->>Cache/DB: Fetch partition A
        Worker2->>Cache/DB: Fetch partition B
    end
    
    Worker1->>Coordinator: Results A
    Worker2->>Coordinator: Results B
    Coordinator->>Coordinator: Combine & Finialize
    Coordinator->>Client: Final Results
```

**Example Query Execution:**
1. User submits: `SELECT * FROM postgres_orders WHERE region = 'EMEA'`
2. Coordinator checks catalogâ€”discovers cached, up-to-date data available
3. DataFusion planner creates optimized physical execution plan
4. Scheduler distributes scan tasks across available workers
5. Workers execute in parallel, filtering for 'EMEA' region
6. Results stream back through Coordinator to client

---

## ğŸ“ Repository Structure

```
igloo/
â”œâ”€â”€ ğŸ“¡ api/                    # Protocol Buffers definitions
â”œâ”€â”€ ğŸ¦€ crates/                 # Core Rust packages
â”‚   â”œâ”€â”€ igloo-coordinator/     # ğŸ§  Coordinator node logic
â”‚   â”œâ”€â”€ igloo-worker/          # ğŸ‘· Worker node implementation  
â”‚   â”œâ”€â”€ igloo-engine/          # âš™ï¸ Core query processing (DataFusion)
â”‚   â”œâ”€â”€ igloo-cache/           # ğŸ’¾ Caching layer
â”‚   â””â”€â”€ connectors/            # ğŸ”Œ Data source plugins
â”œâ”€â”€ ğŸ python/                 # Python bindings
â”œâ”€â”€ ğŸ“š docs/                   # Documentation & design decisions
â””â”€â”€ ğŸ’¡ examples/               # Sample code & tutorials
```

---

## ğŸš€ Getting Started

### Option 1: Docker Compose (Recommended)

The easiest way to get Igloo running with all dependencies:

```bash
# Start Igloo cluster with PostgreSQL
docker-compose up -d --build

# View logs
docker-compose logs -f igloo

# Stop services
docker-compose down
```

### Option 2: Local Development

For development and customization:

1. **Prerequisites:**
   * Rust toolchain (see `rust-toolchain.toml`)
   * Protocol Buffers Compiler (`protoc`)
   * Running PostgreSQL instance
   * ADBC drivers (see environment configuration below)

2. **Configure Environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your database connections and paths
   ```

3. **Build and Run:**
   ```bash
   cargo build --release
   cargo run
   ```

---

## âš™ï¸ Configuration

Igloo is configured via environment variables. Copy `.env.example` to `.env` for local development.

### ğŸ”— Database Connections
```bash
# Primary PostgreSQL connection
DATABASE_URL=postgres://user:password@localhost:5432/mydb

# Alternative format
IGLOO_POSTGRES_URI=host=localhost user=postgres password=postgres dbname=mydb
```

### ğŸ“‚ Data Paths
```bash
# Parquet/Iceberg data location
IGLOO_PARQUET_PATH=./dummy_iceberg_cdc/

# CDC monitoring path
IGLOO_CDC_PATH=./dummy_iceberg_cdc/
```

### ğŸ”§ ADBC Drivers (Local Development)
```bash
# Required for local execution (not needed in Docker)
export LD_LIBRARY_PATH=/path/to/adbc/drivers:$LD_LIBRARY_PATH

# For integration tests
TEST_ADBC_POSTGRESQL_URI=postgresql://user:password@localhost:5432/test_db
```

---

## ğŸ’» Example Usage

### Rust API
```rust
use igloo::{Coordinator, WorkerConfig};
use tokio;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Start coordinator
    let coordinator = Coordinator::new("0.0.0.0:50051").await?;
    
    // Execute federated query
    let results = coordinator.execute_sql(
        "SELECT o.order_id, c.customer_name 
         FROM postgres_orders o 
         JOIN lakehouse_customers c ON o.customer_id = c.id 
         WHERE o.created_at > '2024-01-01'"
    ).await?;
    
    println!("Query results: {:?}", results);
    Ok(())
}
```

### Python Bindings
```python
import igloo

# Connect to Igloo cluster
client = igloo.connect("grpc://localhost:50051")

# Execute SQL with automatic caching
df = client.sql("""
    SELECT region, SUM(revenue) as total_revenue
    FROM sales_data 
    WHERE date >= '2024-01-01'
    GROUP BY region
    ORDER BY total_revenue DESC
""")

print(df.to_pandas())
```

---

## ğŸ¯ Current Features

* âš¡ **Fast SQL Execution** with Apache DataFusion
* ğŸ™ **Distributed Processing** across multiple nodes  
* ğŸ§Š **Smart Result Caching** with query fingerprinting
* ğŸ”„ **CDC-Driven Cache Invalidation** from Iceberg change logs
* ğŸ”— **Cross-Source Joins** between PostgreSQL and Arrow datasets
* ğŸ›¡ï¸ **Memory Safety** guaranteed by Rust
* ğŸ“Š **Arrow Flight SQL** for high-performance client communication

---

## ğŸ›¤ï¸ Roadmap

### Near Term
- [ ] ğŸŒ **REST API** for easier client integration
- [ ] â±ï¸ **Async CDC Updates** with live cache refresh  
- [ ] ğŸ“Š **Query Metrics** (Prometheus, OpenTelemetry)
- [ ] ğŸ”§ **Enhanced Connector Framework**

### Future Vision  
- [ ] ğŸ§  **ML-Powered Query Optimization**
- [ ] ğŸŒ **Multi-Region Deployments**
- [ ] ğŸ“¦ **Persistent Cache Backends** (RocksDB, Redis)
- [ ] ğŸ” **Advanced Security & Auth**
- [ ] ğŸ“ˆ **Auto-scaling** based on query patterns

---

## ğŸ¤ Contributing

We welcome contributions! Whether you're fixing bugs, adding features, or improving documentation:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Run the test suite: `cargo test`
5. Submit a pull request

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ About Igloo

Igloo makes data access simple by bridging the gap between operational databases and analytical workloads. Built by developers who understand the pain of slow, complex data pipelines, Igloo provides the performance and simplicity your team needs to focus on insights, not infrastructure.

**Star â­ this repository if Igloo helps power your data journey!**
